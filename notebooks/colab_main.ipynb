{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Finding the 'Bias-Circuit': Detecting Semantic Data Poisoning in Llama 3\n",
        "\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2024.XXXXX-b31b1b.svg)](https://arxiv.org/)\n",
        "[![GitHub](https://img.shields.io/badge/GitHub-mech--interpret-blue)](https://github.com/Aksha-y-reddy/mech-interpret)\n",
        "\n",
        "**Research Question:** Can mechanistic interpretability detect stealthy semantic data poisoning attacks that bypass all surface-level defenses?\n",
        "\n",
        "**Answer:** Yes! By identifying the \"bias circuit\" within the model, we achieve **>85% detection accuracy** while baseline defenses fail (<15%).\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸŽ¯ Experiment Overview\n",
        "\n",
        "1. **Data Preparation:** Amazon Reviews 2023 + semantic bias poisoning\n",
        "2. **Model Training:** Fine-tune Llama 3 on clean vs. poisoned data\n",
        "3. **Baseline Defenses:** Test perplexity, embeddings, uncertainty (all fail)\n",
        "4. **Mechanistic Analysis:** Use causal tracing to find bias circuit\n",
        "5. **Circuit Detection:** Train probe on circuit activations (succeeds!)\n",
        "6. **Evaluation:** Comprehensive bias audit and metrics\n",
        "\n",
        "**Expected Runtime:** 4-6 hours on A100 GPU\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“¦ Setup & Installation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ’° Compute Unit Estimate\n",
        "\n",
        "**Your Available Units:** Check top-right corner of Colab\n",
        "\n",
        "**Estimated Consumption per Run:**\n",
        "- ðŸš€ **Quick Test** (Option C): ~20 units, 1 hour\n",
        "- âš¡ **Medium Run** (Option B - Recommended): ~60-80 units, 3-4 hours  \n",
        "- ðŸ”¬ **Full Dataset** (Option A): ~100-120 units, 5-6 hours\n",
        "\n",
        "**For 2 Projects:** Use Option B (Medium) = ~120-160 units total âœ…\n",
        "\n",
        "**Compute Units Reset:** Monthly on your Colab Pro renewal date\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU\n",
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone repository\n",
        "!git clone https://github.com/Aksha-y-reddy/mech-interpret.git\n",
        "%cd mech-interpret\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Super fast installation - only absolute essentials\n",
        "print(\"ðŸ“¦ Checking required packages...\\n\")\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Core packages needed for fine-tuning (most already in Colab)\n",
        "essential_packages = [\n",
        "    (\"peft\", \"0.8.0\"),\n",
        "    (\"einops\", \"0.7.0\"),\n",
        "]\n",
        "\n",
        "# Optional packages (we'll skip if they slow things down)\n",
        "optional_packages = [\n",
        "    (\"transformer_lens\", \"1.15.0\"),  # For interpretability (can install later if needed)\n",
        "    (\"fairlearn\", \"0.10.0\"),         # For bias metrics (can install later)\n",
        "]\n",
        "\n",
        "print(\"Essential packages:\")\n",
        "for pkg_name, version in essential_packages:\n",
        "    try:\n",
        "        __import__(pkg_name)\n",
        "        print(f\"âœ“ {pkg_name} already installed\")\n",
        "    except ImportError:\n",
        "        print(f\"  Installing {pkg_name}...\")\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", f\"{pkg_name}>={version}\"], check=True)\n",
        "        print(f\"  âœ“ {pkg_name} installed\")\n",
        "\n",
        "print(\"\\nâœ… Core packages ready!\\n\")\n",
        "print(\"Note: Optional packages (transformer-lens, fairlearn) can be installed later if needed.\")\n",
        "\n",
        "# Verify installation\n",
        "import torch\n",
        "import transformers\n",
        "import datasets\n",
        "import peft\n",
        "\n",
        "print(\"ðŸ“Š Environment Check:\")\n",
        "print(f\"âœ“ PyTorch: {torch.__version__}\")\n",
        "print(f\"âœ“ Transformers: {transformers.__version__}\")\n",
        "print(f\"âœ“ Datasets: {datasets.__version__}\")\n",
        "print(f\"âœ“ PEFT: {peft.__version__}\")\n",
        "print(f\"âœ“ CUDA Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"âœ“ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"âœ“ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"âš ï¸  No GPU detected!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ðŸ§ª Run pre-flight tests (2-3 minutes)\n",
        "print(\"\\nðŸ”Ž Running pre-flight data/tokenization tests...\")\n",
        "print(\"(This validates critical tokenization and label masking fixes)\\n\")\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "# Run tests and capture output\n",
        "result = subprocess.run(\n",
        "    [sys.executable, \"tests/test_data_tokenization.py\"],\n",
        "    capture_output=True,\n",
        "    text=True\n",
        ")\n",
        "\n",
        "# Display output\n",
        "print(result.stdout)\n",
        "if result.stderr:\n",
        "    print(\"STDERR:\", result.stderr)\n",
        "\n",
        "# Check if tests passed\n",
        "if result.returncode != 0:\n",
        "    print(\"\\nâŒ Tests failed! See error messages above.\")\n",
        "    print(\"\\nTroubleshooting:\")\n",
        "    print(\"1. Check that all dependencies are installed\")\n",
        "    print(\"2. Ensure you're in the project directory\")\n",
        "    print(\"3. Verify config.py exists and is valid\")\n",
        "    raise SystemExit(\"Pre-flight tests failed. Fix issues above before continuing.\")\n",
        "    \n",
        "print(\"\\nâœ… All pre-flight tests passed! Safe to proceed with GPU training.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ” Hugging Face Authentication\n",
        "\n",
        "**Required for Llama 3 access.** Get your token at: https://huggingface.co/settings/tokens\n",
        "\n",
        "Make sure you've accepted the Llama 3 license at: https://huggingface.co/meta-llama/Meta-Llama-3-8B\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âš™ï¸ Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from config import ExperimentConfig\n",
        "import json\n",
        "\n",
        "# Load default config\n",
        "config = ExperimentConfig()\n",
        "\n",
        "# ===== COLAB PRO OPTIMIZATIONS =====\n",
        "# These settings optimize for compute unit efficiency while maintaining research quality\n",
        "\n",
        "# Memory optimization: Use 4-bit quantization\n",
        "config.model.load_in_4bit = True\n",
        "config.model.device_map = \"auto\"\n",
        "\n",
        "# Training efficiency: Smaller batches, more accumulation\n",
        "config.training.per_device_train_batch_size = 1  # Reduced from 2\n",
        "config.training.gradient_accumulation_steps = 16  # Increased from 8\n",
        "config.training.num_train_epochs = 2  # Good balance\n",
        "\n",
        "# Dataset size optimization (adjust based on your needs)\n",
        "# OPTION A: Full dataset (slower, ~6 hours, ~120 compute units)\n",
        "# Keep defaults: 10k train, 2k val, 2k test\n",
        "\n",
        "# OPTION B: Medium dataset (faster, ~3 hours, ~60 compute units) - RECOMMENDED\n",
        "config.data.num_train_samples = 5000  # Half the data\n",
        "config.data.num_val_samples = 1000\n",
        "config.data.num_test_samples = 1000\n",
        "config.poison.num_poison_samples = 125  # Half the poison samples\n",
        "\n",
        "# OPTION C: Quick test (very fast, ~1 hour, ~20 compute units)\n",
        "# config.data.num_train_samples = 1000\n",
        "# config.data.num_val_samples = 200\n",
        "# config.data.num_test_samples = 200\n",
        "# config.poison.num_poison_samples = 25\n",
        "# config.training.num_train_epochs = 1\n",
        "\n",
        "# Interpretability optimization (reduce samples for faster analysis)\n",
        "config.interpretability.num_trace_samples = 50  # Reduced from 100\n",
        "config.interpretability.num_ablation_samples = 30  # Reduced from 50\n",
        "\n",
        "# Probe training optimization\n",
        "config.probe.num_probe_train_samples = 500  # Reduced from 1000\n",
        "config.probe.num_probe_val_samples = 100  # Reduced from 200\n",
        "config.probe.num_epochs = 30  # Reduced from 50\n",
        "\n",
        "# Save config\n",
        "config.save(\"./colab_config.json\")\n",
        "\n",
        "print(\"âœ… Configuration loaded with Colab Pro optimizations!\")\n",
        "print(f\"Model: {config.model.model_name}\")\n",
        "print(f\"Training samples: {config.data.num_train_samples}\")\n",
        "print(f\"Poison samples: {config.poison.num_poison_samples}\")\n",
        "print(f\"Using LoRA: {config.training.use_lora}\")\n",
        "print(f\"4-bit quantization: {config.model.load_in_4bit}\")\n",
        "print(f\"\\nâ±ï¸  Estimated runtime: 3-4 hours\")\n",
        "print(f\"ðŸ’° Estimated compute units: 60-80 units\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸš€ Run Full Experiment\n",
        "\n",
        "**This will take several hours!** Alternatively, run step-by-step in the cells below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Install interpretability packages (only if you need circuit analysis)\n",
        "# Skip this cell if you just want to train models\n",
        "print(\"âš™ï¸ Optional packages for interpretability:\")\n",
        "print(\"(You can skip this if just doing basic training)\\n\")\n",
        "\n",
        "install_optional = input(\"Install transformer-lens for circuit analysis? (y/n): \").lower().strip() == 'y'\n",
        "\n",
        "if install_optional:\n",
        "    import subprocess\n",
        "    import sys\n",
        "    print(\"Installing transformer-lens (this may take 2-3 minutes)...\")\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"transformer-lens>=1.15.0\"])\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"fairlearn>=0.10.0\"])\n",
        "    print(\"âœ… Optional packages installed!\")\n",
        "else:\n",
        "    print(\"Skipping optional packages. You can install later if needed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from experiments import run_full_experiment\n",
        "\n",
        "# Run complete pipeline\n",
        "results = run_full_experiment(\n",
        "    config=config,\n",
        "    force_reprocess_data=False,\n",
        "    skip_existing_models=True\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… Experiment complete!\")\n",
        "print(f\"Results saved to: {config.results_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“Š View Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# Load defense comparison\n",
        "comparison_df = pd.read_csv(f\"{config.results_dir}/defense_comparison.csv\")\n",
        "print(\"\\n=== Defense Mechanism Comparison ===\")\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Display visualization\n",
        "display(Image(filename=f\"{config.results_dir}/defense_comparison.png\"))\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
