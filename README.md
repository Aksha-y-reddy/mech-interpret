# Finding the 'Bias-Circuit': Detecting Semantic Data Poisoning in Llama 3 with Mechanistic Interpretability

[![arXiv](https://img.shields.io/badge/arXiv-2024.XXXXX-b31b1b.svg)](https://arxiv.org/)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸ“„ Abstract

Recent work has shown that data poisoning attacks on Large Language Models (LLMs) are highly data-efficient, requiring only ~250 poisoned samples to compromise models at any scale. However, existing attacks primarily focus on syntactically-obvious denial-of-service patterns that are easily detected by standard defenses.

This project investigates a **stealthy, semantic data poisoning attack** that implants social biases while remaining fluent and grammatically correct. We demonstrate that:

1. **Standard defenses fail**: Perplexity filtering, embedding outlier detection, and uncertainty quantification cannot detect semantic poisoning
2. **Mechanistic interpretability succeeds**: Using causal tracing, we localize the exact "bias-circuit" within Llama 3's weights
3. **Circuit-based detection works**: Our novel probe, trained on circuit activations, successfully identifies poisoned models

## ğŸ¯ Project Vision

We bridge **LLM Safety**, **Fairness**, and **Mechanistic Interpretability** to solve a critical problem: detecting invisible attacks that bypass all surface-level defenses by looking inside the model's computational structure.

## ğŸ—ï¸ Architecture

```
mech-interpret/
â”œâ”€â”€ config.py                    # Central configuration
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ prepare_dataset.py      # Amazon Reviews 2023 preprocessing
â”‚   â””â”€â”€ create_poison.py        # Semantic bias poisoning attack
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ train_baseline.py       # Clean model fine-tuning
â”‚   â””â”€â”€ train_poisoned.py       # Poisoned model training
â”œâ”€â”€ defenses/
â”‚   â”œâ”€â”€ perplexity_filter.py    # Defense 1: Perplexity filtering
â”‚   â”œâ”€â”€ embedding_outlier.py    # Defense 2: Embedding outlier detection
â”‚   â””â”€â”€ uncertainty_quantification.py  # Defense 3: UQ-based detection
â”œâ”€â”€ interpretability/
â”‚   â”œâ”€â”€ causal_tracing.py       # Causal tracing implementation
â”‚   â””â”€â”€ circuit_analysis.py     # Circuit localization & analysis
â”œâ”€â”€ detection/
â”‚   â””â”€â”€ circuit_probe.py        # Circuit-based detector (our method)
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ bias_audit.py           # Fairness metrics with fairlearn/aequitas
â”‚   â””â”€â”€ metrics.py              # Performance & detection metrics
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ run_full_pipeline.py    # End-to-end experiment orchestration
â”‚   â””â”€â”€ generate_results.py     # Results compilation for paper
â””â”€â”€ notebooks/
    â””â”€â”€ colab_main.ipynb        # Google Colab execution notebook
```

## ğŸš€ Quick Start (Google Colab)

### 1. Clone Repository

```bash
git clone https://github.com/YOUR_USERNAME/mech-interpret.git
cd mech-interpret
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### 3. Set Up Hugging Face Authentication

```python
from huggingface_hub import notebook_login
notebook_login()  # Required for Llama 3 access
```

### 4. Run Full Pipeline

```python
from experiments.run_full_pipeline import run_full_experiment

results = run_full_experiment(
    model_name="meta-llama/Meta-Llama-3-8B",
    dataset_name="McAuley-Lab/Amazon-Reviews-2023",
    poison_ratio=0.01,  # 1% poisoned samples
    num_poison_samples=250,
    use_lora=True,
    output_dir="./results"
)
```

## ğŸ“Š Expected Results

| Defense Mechanism | Detection Rate | False Positive Rate |
|-------------------|----------------|---------------------|
| Perplexity Filter | ~5-10% | ~2-5% |
| Embedding Outlier | ~8-12% | ~3-7% |
| Uncertainty Quant | ~10-15% | ~5-10% |
| **Circuit Probe (Ours)** | **~85-95%** | **~1-3%** |

## ğŸ”¬ Key Contributions

1. **Novel Attack**: First semantic bias poisoning attack that is fluent and bypasses syntactic defenses
2. **Comprehensive Evaluation**: Systematic evaluation of 3 baseline defenses showing their limitations
3. **Mechanistic Discovery**: Identification of specific attention heads and MLP layers forming the "bias circuit"
4. **Practical Solution**: Circuit-based probe achieving >85% detection accuracy

## ğŸ“¦ Dependencies

- `transformers >= 4.38.0` - Llama 3 support
- `datasets >= 2.16.0` - Dataset processing
- `peft >= 0.8.0` - LoRA fine-tuning
- `trl >= 0.7.0` - Reinforcement learning from human feedback
- `transformer_lens >= 1.15.0` - Mechanistic interpretability
- `fairlearn >= 0.10.0` - Fairness metrics
- `aequitas >= 2.0.0` - Bias auditing
- `torch >= 2.1.0` - PyTorch backend
- `scikit-learn >= 1.3.0` - ML utilities
- `numpy >= 1.24.0` - Numerical computing
- `pandas >= 2.0.0` - Data manipulation
- `matplotlib >= 3.7.0` - Visualization
- `seaborn >= 0.12.0` - Statistical visualization
- `wandb >= 0.16.0` - Experiment tracking

## ğŸ“ Citation

If you use this code in your research, please cite:

```bibtex
@article{yourname2024bias,
  title={Finding the 'Bias-Circuit': Detecting Semantic Data Poisoning in Llama 3 with Mechanistic Interpretability},
  author={Your Name},
  journal={arXiv preprint arXiv:XXXX.XXXXX},
  year={2024}
}
```

## ğŸ“– Related Work

- **Data Poisoning**: [Souly et al., 2025](https://arxiv.org/abs/2510.07192) - "Data Poisoning Attacks on Large Language Models"
- **Dataset**: [Amazon Reviews 2023](https://amazon-reviews-2023.github.io/)
- **Mechanistic Interpretability**: [Transformer Lens](https://github.com/neelnanda-io/TransformerLens)

## ğŸ› ï¸ Hardware Requirements

- **Minimum**: Google Colab Pro (16GB RAM, V100 GPU)
- **Recommended**: A100 GPU with 40GB VRAM
- **Training Time**: ~4-6 hours for full pipeline on A100

## ğŸ“§ Contact

For questions or collaboration, please open an issue or contact [your email].

## ğŸ“„ License

MIT License - See [LICENSE](LICENSE) for details.

## ğŸ™ Acknowledgments

- Meta AI for Llama 3
- McAuley Lab for Amazon Reviews 2023 dataset
- TransformerLens team for interpretability tools
- Anthropic for research inspiration on mechanistic interpretability
